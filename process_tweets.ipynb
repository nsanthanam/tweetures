{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "pd.options.display.max_colwidth = 150\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "tagkeys = tagdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_data(filename):\n",
    "    \"\"\"\n",
    "    This function takes in a filename and opens the file in read mode. Using the format provided in the sample CSV\n",
    "    file, it removes extraneous characters and generates a list of lists where each list is a single row of the \n",
    "    input file.\n",
    "    \n",
    "    This list of lists is then converted into a Pandas DataFrame with the tweet id as the index.\n",
    "    \n",
    "    Input: fn (string)\n",
    "    Output: tweets (Pandas DataFrame)\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(filename, 'r') as fd:\n",
    "            for line in fd:\n",
    "                split_line = line.split(';') #columns are separated by semi-colons\n",
    "\n",
    "                #remove extraneous characters from the line\n",
    "                split_line2 = [item.replace('\"', \"\") for item in split_line] \n",
    "                split_line2 = [item.replace('\\r', \"\") for item in split_line2]\n",
    "                split_line2 = [item.replace('\\n', \"\") for item in split_line2]\n",
    "                split_line2 = [re.sub(pattern=',+$', repl='', string=item) for item in split_line2] #remove all commas at end of line\n",
    "\n",
    "                #tweets containing a comma are split into multiple strings in the list - here we concatenate them into one string\n",
    "                if len(split_line2) > 3:\n",
    "                    for item in range(3, len(split_line2)):\n",
    "                        split_line2[2] = split_line2[2] + split_line2[item]\n",
    "                    split_line2 = split_line2[0:3]\n",
    "                lines.append(split_line2) #append each line to list\n",
    "\n",
    "        #Convert list into dataframe\n",
    "        tweets = pd.DataFrame(lines, columns=['id', 'polarity', 'tweet'])\n",
    "        tweets.drop(labels=0, axis=0, inplace=True) #first list row is the header, dropping it\n",
    "\n",
    "        tweets['id'] = tweets['id'].astype(int)\n",
    "        tweets.set_index(keys='id', inplace=True) #the tweet id is being made the index\n",
    "        tweets['polarity'] = tweets['polarity'].astype(int)\n",
    "        \n",
    "        return tweets\n",
    "    except IOError, e:\n",
    "        print 'File not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(DF):\n",
    "    \"\"\"\n",
    "    Generates features based on the tweets DataFrame. Calculates how many mentions, hashtags, and words in a tweet. \n",
    "    Also counts the different parts of speech each tweet contains.\n",
    "    \n",
    "    Input: tweets DataFrame\n",
    "    Output: features DataFrame\n",
    "    \"\"\"\n",
    "    #basic features such as the number of '@'-mentions, hashtags, and number of words in the tweet\n",
    "    DF['mentions'] = DF['tweet'].apply(lambda s: len(re.findall(pattern= '@[a-z]+', string=s)))\n",
    "    DF['hashtags'] = DF['tweet'].apply(lambda s: len(re.findall(pattern= '#[a-z]+', string=s)))\n",
    "    tokenized = tweets['tweet'].apply(nltk.word_tokenize) #tokenise tweets into list of words\n",
    "    DF['wordcount'] = tokenized.apply(len) #number of words in tweet\n",
    "    \n",
    "    #categorise tokens into relevant part of speech and store in DataFrame\n",
    "    pos_list = tokenized.apply(lambda t: map(lambda x: x[1], nltk.pos_tag(t)))\n",
    "    pos_dict = pos_list.apply(lambda x: Counter(x)) #Count how many of each type there are\n",
    "    \n",
    "    #convert the list of dicts into a dataframe\n",
    "    pos_df = pd.DataFrame(columns=[], index=pos_dict.index.values, data=0)\n",
    "    for i in pos_dict.index.values:\n",
    "        for j in pos_dict[i].keys():\n",
    "            pos_df.loc[i, j] = pos_dict[i][j]\n",
    "    pos_df.replace(to_replace=np.nan, value=0, inplace=True) #replace any NaNs with 0 (meaning that part of speech wasn't in the tweet)\n",
    "    pos_df = pos_df.astype(int) #convert everything to integer\n",
    "\n",
    "    #merge the DataFrame with basic features and the parts of speech counts\n",
    "    features = pd.merge(DF, pos_df, left_index=True, right_index=True)\n",
    "    features.drop(labels=['tweet'], axis=1, inplace=True) #drop the original tweet text\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = '/Users/navaneethan/Dropbox/projects/freshdesk/data/sts_gold_tweet.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = preprocess_data(filename=fn)\n",
    "features = generate_features(DF=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    input_file_path = os.path.dirname(os.path.abspath(fn))\n",
    "    output_filename = input_file_path + '/' + 'tweet_features.csv'\n",
    "    features.to_csv(output_filename)\n",
    "except IOError, e:\n",
    "    print 'Unable to write'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
